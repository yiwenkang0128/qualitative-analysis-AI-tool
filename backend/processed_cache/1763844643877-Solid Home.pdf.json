{"full_text": "Project Charter\nDate Modified: 10/20/2025\nTeam Name: Solid Home\nTeam Members: Zhen Liang, Renjie Zhang, Yiwen Kang\nProject Topic Title: Automating Anti-Displacement Qualitative Research\nProblem Statement\nThe City of Seattle recently established an Anti-Displacement Workgroup to better align\nincome levels with rental prices and reduce the risk of residents being priced out of\nHowever, the current policy and planning documents are mostly in lengthy PDFs, which\nare time-consuming to review and inconsistent across chapters. This makes it difficult\nfor users to quickly grasp policy content and hinders cross-department collaboration.\nAt the same time, the City faces similar challenges in other long-term initiatives—such\nas climate and transportation—so the client hopes to develop reusable and auditable\ntext-analysis tools that can support policy decisions and external communication.\nGoal Statement\nThe goal of this project is to develop an automated survey analysis model leveraging\nLarge Language Models. By defining and extracting key tokens, the model will\nstreamline the process of reading. Ultimately, the project aims to improve the efficiency\nand accuracy of survey analysis, enabling stakeholders to make more informed,\ndata-driven decisions.\nSpecific: Build an end-to-end pipeline: PDF → structured text → semantic chunking →\nembeddings → BERTopic clustering → LLM topic naming/summaries → visualization.\nMeasurable:\n● Parsing coverage ≥ 95% (readable pages / total pages); if OCR is triggered,\nOCR F1 ≥ 0.90.\n● Topic coherence ≥ 0.45\n● Executive summary hit-rate ≥ 80%\nAchievable: Implemented with PyPDF/Unstructured + UMAP/HDBSCAN/BERTopic +\nGPT-series LLMs; technically feasible.\nRelevant: Focus on comprehensive-plan themes (housing/anti-displacement/growth)\naligned to city governance needs.\nTime-Bound\n● Week 6: Data review\n● Week 7: Model Design\n● Week 10: V1 finish\n● Week 11: Client feedback\n● Week 12: Modify\n● Week 13: Final version\n1.Data Ingestion & Preprocessing\n● Batch PDF ingestion; layout parsing (text & tables as text); optional OCR for\nscanned pages.\n● Layout noise cleanup: TOC, headers/footers, footnotes; hyphenation/line-break\nrepair; table rule noise suppression.\n● Semantic chunking (title/section-aware + length/overlap windows); near-duplicate\nmerging (MinHash/SimHash).\n2.Modeling & topic engineering\n● Embeddings; dimensionality reduction (UMAP); density clustering (HDBSCAN);\nBERTopic keywords.\n● Topic quality evaluation (coherence; sampled fidelity review).\n3.Outputs & visualization\n● CSV/Parquet exports: topic inventory, keywords, exemplar snippets,\ndocument–topic soft distributions.\n● Visuals: top-N topics, document coverage heatmap, importance bars/scatter,\ntopic relationship graph.\n● One-Pager executive summary + appendix (methods, metrics, reproducibility\nOut-of-Scope:\n● Deep image/table number extraction or figure semantics\n● External fact-checking\n● Multi-tenant online platform\nStakeholders\n● City of Seattle Program Staff\n● Industry Mentor: Janis Jordan (she/her)\n● Team member: Zhen Liang, Renjie Zhang, Yiwen Kang\nKey Milestones\n● Week 6: Cleaning , chunking, parsing.\n● Week 7: Making a demo and meeting with the client.\n● Week 8: Embedding/clustering/BERTopic baseline.\n● Week 9: Implementing the topic merge strategy.\n● Week 10: Visualization and testing.\n● Week 11: Reviewing by the client and getting feedback.\n● Week 12: The final version is done.\nTeam Members\nYiwen Kang – Ingest & Prepare Text: Build the input pipeline from PDF to clean\nparagraph chunks. Batch-read PDFs, strip headers/footers/TOC/footnotes, fix line\nbreaks/hyphenation, and split into 300–800-character chunks with slight overlap\nZhen Liang – Embed, Cluster & Extract Keywords: Convert chunks to sentence\nembeddings, run a stable clustering method to assign each chunk a topic_id, and\naggregate by topic. Print quick stats and add a simple check that sampled items within a\ntopic look semantically related.\nRenjie Zhang – Name Topics, Summarize & Deliver Outputs: Generate short,\nhuman-readable topic names, write concise per-topic summaries and a one-page\ndocument overview, and create a few essential charts, like topic sizes and topic ×\nsection heatmap).\nSuccess Criteria\n● Parsing coverage ≥ 95%.\n● Topic coherence (c_v/c_npmi) ≥ 0.45.\n● Content duplication ≤ 10%.\n● Improving the qualitative analysis process by at least 20%."}